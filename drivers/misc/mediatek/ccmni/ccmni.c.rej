--- drivers/misc/mediatek/ccmni/ccmni.c
+++ drivers/misc/mediatek/ccmni/ccmni.c
@@ -259,13 +265,13 @@ static u16 ccmni_select_queue(struct net_device *dev, struct sk_buff *skb,
 
 static int ccmni_open(struct net_device *dev)
 {
-	ccmni_instance_t *ccmni = (ccmni_instance_t *)netdev_priv(dev);
-	ccmni_ctl_block_t *ccmni_ctl = ccmni_ctl_blk[ccmni->md_id];
-	ccmni_instance_t *ccmni_tmp = NULL;
+	struct ccmni_instance *ccmni = (struct ccmni_instance *)netdev_priv(dev);
+	struct ccmni_ctl_block *ccmni_ctl = ccmni_ctl_blk[ccmni->md_id];
+	struct ccmni_instance *ccmni_tmp = NULL;
 	int usage_cnt = 0;
 
 	if (unlikely(ccmni_ctl == NULL)) {
-		CCMNI_ERR_MSG(ccmni->md_id, "%s_Open: MD%d ctlb is NULL\n", dev->name, ccmni->md_id);
+		CCMNI_PR_ERR(ccmni->md_id, "%s_Open: MD%d ctlb is NULL\n", dev->name, ccmni->md_id);
 		return -1;
 	}
 
@@ -285,21 +291,22 @@ static int ccmni_open(struct net_device *dev)
 		atomic_set(&ccmni_tmp->usage, usage_cnt);
 	}
 
-	CCMNI_INF_MSG(ccmni->md_id, "%s_Open: cnt=(%d,%d), md_ab=0x%X\n",
-		dev->name, atomic_read(&ccmni->usage),
-		atomic_read(&ccmni_tmp->usage), ccmni_ctl->ccci_ops->md_ability);
+	CCMNI_INF_MSG(ccmni->md_id, "%s_Open:cnt=(%d,%d), md_ab=0x%X, gro=(%llx, %ld), flt_cnt=%d\n",
+		      dev->name, atomic_read(&ccmni->usage),
+		      atomic_read(&ccmni_tmp->usage), ccmni_ctl->ccci_ops->md_ability,
+		      dev->features, gro_flush_timer, ccmni->flt_cnt);
 	return 0;
 }
 
 static int ccmni_close(struct net_device *dev)
 {
-	ccmni_instance_t *ccmni = (ccmni_instance_t *)netdev_priv(dev);
-	ccmni_ctl_block_t *ccmni_ctl = ccmni_ctl_blk[ccmni->md_id];
-	ccmni_instance_t *ccmni_tmp = NULL;
+	struct ccmni_instance *ccmni = (struct ccmni_instance *)netdev_priv(dev);
+	struct ccmni_ctl_block *ccmni_ctl = ccmni_ctl_blk[ccmni->md_id];
+	struct ccmni_instance *ccmni_tmp = NULL;
 	int usage_cnt = 0;
 
 	if (unlikely(ccmni_ctl == NULL)) {
-		CCMNI_ERR_MSG(ccmni->md_id, "%s_Close: MD%d ctlb is NULL\n", dev->name, ccmni->md_id);
+		CCMNI_PR_ERR(ccmni->md_id, "%s_Close: MD%d ctlb is NULL\n", dev->name, ccmni->md_id);
 		return -1;
 	}
 
@@ -325,22 +332,30 @@ static int ccmni_start_xmit(struct sk_buff *skb, struct net_device *dev)
 {
 	int ret;
 	int skb_len = skb->len;
-	ccmni_instance_t *ccmni = (ccmni_instance_t *)netdev_priv(dev);
-	ccmni_ctl_block_t *ctlb = ccmni_ctl_blk[ccmni->md_id];
+	struct ccmni_instance *ccmni = (struct ccmni_instance *)netdev_priv(dev);
+	struct ccmni_ctl_block *ctlb = ccmni_ctl_blk[ccmni->md_id];
 	unsigned int is_ack = 0;
 
+#if defined(CCMNI_MET_DEBUG)
+	char tag_name[32] = { '\0' };
+	unsigned int tag_id = 0;
+#endif
+
+	if (ccmni_forward_rx(ccmni, skb) == NETDEV_TX_OK)
+		return NETDEV_TX_OK;
+
 	/* dev->mtu is changed  if dev->mtu is changed by upper layer */
 	if (unlikely(skb->len > dev->mtu)) {
-		CCMNI_ERR_MSG(ccmni->md_id, "CCMNI%d write fail: len(0x%x)>MTU(0x%x, 0x%x)\n",
-			ccmni->index, skb->len, CCMNI_MTU, dev->mtu);
+		CCMNI_PR_ERR(ccmni->md_id, "CCMNI%d write fail: len(0x%x)>MTU(0x%x, 0x%x)\n",
+			      ccmni->index, skb->len, CCMNI_MTU, dev->mtu);
 		dev_kfree_skb(skb);
 		dev->stats.tx_dropped++;
 		return NETDEV_TX_OK;
 	}
 
 	if (unlikely(skb_headroom(skb) < sizeof(struct ccci_header))) {
-		CCMNI_ERR_MSG(ccmni->md_id, "CCMNI%d write fail: header room(%d) < ccci_header(%d)\n",
-			ccmni->index, skb_headroom(skb), dev->hard_header_len);
+		CCMNI_PR_ERR(ccmni->md_id, "CCMNI%d write fail: header room(%d) < ccci_header(%d)\n",
+			      ccmni->index, skb_headroom(skb), dev->hard_header_len);
 		dev_kfree_skb(skb);
 		dev->stats.tx_dropped++;
 		return NETDEV_TX_OK;
@@ -396,22 +416,22 @@ tx_busy:
 
 static int ccmni_change_mtu(struct net_device *dev, int new_mtu)
 {
-	ccmni_instance_t *ccmni = (ccmni_instance_t *)netdev_priv(dev);
+	struct ccmni_instance *ccmni = (struct ccmni_instance *)netdev_priv(dev);
 
 	if (new_mtu > CCMNI_MTU)
 		return -EINVAL;
 
 	dev->mtu = new_mtu;
-	CCMNI_INF_MSG(ccmni->md_id, "CCMNI%d change mtu_siz=%d\n", ccmni->index, new_mtu);
+	CCMNI_DBG_MSG(ccmni->md_id, "CCMNI%d change mtu_siz=%d\n", ccmni->index, new_mtu);
 	return 0;
 }
 
 static void ccmni_tx_timeout(struct net_device *dev)
 {
-	ccmni_instance_t *ccmni = (ccmni_instance_t *)netdev_priv(dev);
+	struct ccmni_instance *ccmni = (struct ccmni_instance *)netdev_priv(dev);
 
-	CCMNI_INF_MSG(ccmni->md_id, "ccmni%d_tx_timeout: usage_cnt=%d, timeout=%ds\n",
-		ccmni->index, atomic_read(&ccmni->usage), (ccmni->dev->watchdog_timeo/HZ));
+	CCMNI_DBG_MSG(ccmni->md_id, "ccmni%d_tx_timeout: usage_cnt=%d, timeout=%ds\n",
+		      ccmni->index, atomic_read(&ccmni->usage), (ccmni->dev->watchdog_timeo/HZ));
 
 	dev->stats.tx_errors++;
 	if (atomic_read(&ccmni->usage) > 0)
@@ -421,12 +441,16 @@ static void ccmni_tx_timeout(struct net_device *dev)
 static int ccmni_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
 {
 	int md_id, md_id_irat, usage_cnt;
-	ccmni_instance_t *ccmni_irat;
-	ccmni_instance_t *ccmni = (ccmni_instance_t *)netdev_priv(dev);
-	ccmni_instance_t *ccmni_tmp = NULL;
-	ccmni_ctl_block_t *ctlb = NULL;
-	ccmni_ctl_block_t *ctlb_irat = NULL;
+	struct ccmni_instance *ccmni_irat;
+	struct ccmni_instance *ccmni = (struct ccmni_instance *)netdev_priv(dev);
+	struct ccmni_instance *ccmni_tmp = NULL;
+	struct ccmni_ctl_block *ctlb = NULL;
+	struct ccmni_ctl_block *ctlb_irat = NULL;
 	unsigned int timeout = 0;
+	struct ccmni_fwd_filter flt_tmp;
+	struct ccmni_flt_act flt_act;
+	unsigned int i;
+	unsigned int cmp_len;
 
 	switch (cmd) {
 	case SIOCSTXQSTATE:
@@ -497,22 +521,24 @@ static int ccmni_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
 
 		ctlb_irat = ccmni_ctl_blk[md_id_irat];
 		if (ccmni->index >= ctlb_irat->ccci_ops->ccmni_num) {
-			CCMNI_ERR_MSG(md_id, "SIOCSCCMNICFG: %s iRAT(MD%d->MD%d) fail,index(%d)>max_num(%d)\n",
-				dev->name, md_id, md_id_irat, ccmni->index, ctlb_irat->ccci_ops->ccmni_num);
+			CCMNI_PR_ERR(md_id, "SIOCSCCMNICFG: %s iRAT(MD%d->MD%d) fail,index(%d)>max_num(%d)\n",
+				      dev->name, md_id, md_id_irat, ccmni->index,
+				      ctlb_irat->ccci_ops->ccmni_num);
 			break;
 		}
 		ccmni_irat = ctlb_irat->ccmni_inst[ccmni->index];
 
 		if (md_id_irat == ccmni->md_id) {
 			if (ccmni_irat->dev != dev) {
-				CCMNI_INF_MSG(md_id, "SIOCCCMNICFG: %s iRAT on MD%d, diff dev(%s->%s)\n",
-					dev->name, (ifr->ifr_ifru.ifru_ivalue+1), ccmni_irat->dev->name, dev->name);
+				CCMNI_DBG_MSG(md_id, "SIOCCCMNICFG: %s iRAT on MD%d, diff dev(%s->%s)\n",
+					      dev->name, (ifr->ifr_ifru.ifru_ivalue + 1),
+					      ccmni_irat->dev->name, dev->name);
 				ccmni_irat->dev = dev;
 				usage_cnt = atomic_read(&ccmni->usage);
 				atomic_set(&ccmni_irat->usage, usage_cnt);
 			} else
-				CCMNI_INF_MSG(md_id, "SIOCCCMNICFG: %s iRAT on the same MD%d, cnt=%d\n",
-					dev->name, (ifr->ifr_ifru.ifru_ivalue+1), atomic_read(&ccmni->usage));
+				CCMNI_DBG_MSG(md_id, "SIOCCCMNICFG: %s iRAT on the same MD%d, cnt=%d\n",
+					      dev->name, (ifr->ifr_ifru.ifru_ivalue + 1), atomic_read(&ccmni->usage));
 			break;
 		}
 
@@ -527,16 +553,78 @@ static int ccmni_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
 		/* fix dev!=ccmni_irat->dev issue when MD3-CC3MNI -> MD3-CCMNI */
 		ccmni_irat->dev = dev;
 		atomic_set(&ccmni_irat->usage, usage_cnt);
-		memcpy(netdev_priv(dev), ccmni_irat, sizeof(ccmni_instance_t));
+		memcpy(netdev_priv(dev), ccmni_irat, sizeof(struct ccmni_instance));
 
-		CCMNI_INF_MSG(md_id,
-			"SIOCCCMNICFG: %s iRAT MD%d->MD%d, dev_cnt=%d, md_cnt=%d, md_irat_cnt=%d, irat_dev=%s\n",
-			dev->name, (md_id+1), (ifr->ifr_ifru.ifru_ivalue+1), atomic_read(&ccmni->usage),
-			atomic_read(&ccmni_tmp->usage), atomic_read(&ccmni_irat->usage), ccmni_irat->dev->name);
+		CCMNI_DBG_MSG(md_id,
+			      "SIOCCCMNICFG: %s iRAT MD%d->MD%d, dev_cnt=%d, md_cnt=%d, md_irat_cnt=%d, irat_dev=%s\n",
+			      dev->name, (md_id + 1), (ifr->ifr_ifru.ifru_ivalue + 1), atomic_read(&ccmni->usage),
+			      atomic_read(&ccmni_tmp->usage), atomic_read(&ccmni_irat->usage), ccmni_irat->dev->name);
+		break;
+
+	case SIOCFWDFILTER:
+		if (copy_from_user(&flt_act, ifr->ifr_ifru.ifru_data, sizeof(struct ccmni_flt_act))) {
+			CCMNI_INF_MSG(ccmni->md_id, "SIOCFWDFILTER: %s copy data from user fail\n", dev->name);
+			return -EFAULT;
+		}
+
+		flt_tmp = flt_act.flt;
+		if (flt_tmp.ver != 0x40 && flt_tmp.ver != 0x60) {
+			CCMNI_INF_MSG(ccmni->md_id, "SIOCFWDFILTER[%d]: %s invalid flt(%x, %x, %x, %x, %x)(%d)\n",
+				flt_act.action, dev->name, flt_tmp.ver, flt_tmp.s_pref, flt_tmp.d_pref,
+				flt_tmp.ipv4.saddr, flt_tmp.ipv4.daddr, ccmni->flt_cnt);
+			return -EINVAL;
+		}
+
+		if (flt_act.action == CCMNI_FLT_ADD) { /* add new filter */
+			if (ccmni->flt_cnt >= CCMNI_FLT_NUM) {
+				CCMNI_INF_MSG(ccmni->md_id, "SIOCFWDFILTER[ADD]: %s flt table full\n", dev->name);
+				return -ENOMEM;
+			}
+			for (i = 0; i < CCMNI_FLT_NUM; i++) {
+				if (ccmni->flt_tbl[i].ver == 0)
+					break;
+			}
+			if (i < CCMNI_FLT_NUM) {
+				memcpy(&ccmni->flt_tbl[i], &flt_tmp, sizeof(struct ccmni_fwd_filter));
+				ccmni->flt_cnt++;
+			}
+			CCMNI_INF_MSG(ccmni->md_id, "SIOCFWDFILTER[ADD]: %s add flt%d(%x, %x, %x, %x, %x)(%d)\n",
+				dev->name, i, flt_tmp.ver, flt_tmp.s_pref, flt_tmp.d_pref,
+				flt_tmp.ipv4.saddr, flt_tmp.ipv4.daddr, ccmni->flt_cnt);
+		} else if (flt_act.action == CCMNI_FLT_DEL) {
+			if (flt_tmp.ver == IPV4_VERSION)
+				cmp_len = offsetof(struct ccmni_fwd_filter, ipv4.daddr) + 4;
+			else
+				cmp_len = sizeof(struct ccmni_fwd_filter);
+			for (i = 0; i < CCMNI_FLT_NUM; i++) {
+				if (ccmni->flt_tbl[i].ver == 0)
+					continue;
+				if (!memcmp(&ccmni->flt_tbl[i], &flt_tmp, cmp_len)) {
+					CCMNI_INF_MSG(ccmni->md_id,
+						"SIOCFWDFILTER[DEL]: %s del flt%d(%x, %x, %x, %x, %x)(%d)\n",
+						dev->name, i, flt_tmp.ver, flt_tmp.s_pref, flt_tmp.d_pref,
+						flt_tmp.ipv4.saddr, flt_tmp.ipv4.daddr, ccmni->flt_cnt);
+					memset(&ccmni->flt_tbl[i], 0, sizeof(struct ccmni_fwd_filter));
+					ccmni->flt_cnt--;
+					break;
+				}
+			}
+			if (i >= CCMNI_FLT_NUM) {
+				CCMNI_INF_MSG(ccmni->md_id,
+					"SIOCFWDFILTER[DEL]: %s no match flt(%x, %x, %x, %x, %x)(%d)\n",
+					dev->name, flt_tmp.ver, flt_tmp.s_pref, flt_tmp.d_pref,
+					flt_tmp.ipv4.saddr, flt_tmp.ipv4.daddr, ccmni->flt_cnt);
+				return -ENXIO;
+			}
+		} else if (flt_act.action == CCMNI_FLT_FLUSH) {
+			ccmni->flt_cnt = 0;
+			memset(ccmni->flt_tbl, 0, CCMNI_FLT_NUM*sizeof(struct ccmni_fwd_filter));
+			CCMNI_INF_MSG(ccmni->md_id, "SIOCFWDFILTER[FLUSH]: %s flush filter\n", dev->name);
+		}
 		break;
 
 	default:
-		CCMNI_ERR_MSG(ccmni->md_id, "%s: unknown ioctl cmd=%x\n", dev->name, cmd);
+		CCMNI_DBG_MSG(ccmni->md_id, "%s: unknown ioctl cmd=%x\n", dev->name, cmd);
 		break;
 	}
 
@@ -558,9 +646,9 @@ static int ccmni_napi_poll(struct napi_struct *napi, int budget)
 #ifdef ENABLE_WQ_GRO
 	return 0;
 #else
-	ccmni_instance_t *ccmni = (ccmni_instance_t *)netdev_priv(napi->dev);
+	struct ccmni_instance *ccmni = (struct ccmni_instance *)netdev_priv(napi->dev);
 	int md_id = ccmni->md_id;
-	ccmni_ctl_block_t *ctlb = ccmni_ctl_blk[md_id];
+	struct ccmni_ctl_block *ctlb = ccmni_ctl_blk[md_id];
 
 	del_timer(ccmni->timer);
 
@@ -616,147 +704,147 @@ static inline int ccmni_inst_init(int md_id, ccmni_instance_t *ccmni, struct net
 	return ret;
 }
 
-static int ccmni_init(int md_id, ccmni_ccci_ops_t *ccci_info)
+static inline void ccmni_dev_init(int md_id, struct net_device *dev)
+{
+	struct ccmni_ctl_block *ctlb = ccmni_ctl_blk[md_id];
+
+	dev->header_ops = NULL;
+	dev->mtu = CCMNI_MTU;
+	dev->tx_queue_len = CCMNI_TX_QUEUE;
+	dev->watchdog_timeo = CCMNI_NETDEV_WDT_TO;
+	/* ccmni is a pure IP device */
+	dev->flags = IFF_NOARP &
+			(~IFF_BROADCAST & ~IFF_MULTICAST);
+	/* not support VLAN */
+	dev->features = NETIF_F_VLAN_CHALLENGED;
+	if (ctlb->ccci_ops->md_ability & MODEM_CAP_SGIO) {
+		dev->features |= NETIF_F_SG;
+		dev->hw_features |= NETIF_F_SG;
+	}
+	if (ctlb->ccci_ops->md_ability & MODEM_CAP_NAPI) {
+#ifdef ENABLE_NAPI_GRO
+		dev->features |= NETIF_F_GRO;
+		dev->hw_features |= NETIF_F_GRO;
+#else
+		/*
+		 * check gro_list_prepare, GRO needs hard_header_len == ETH_HLEN.
+		 * CCCI header can use ethernet header and padding bytes' region.
+		 */
+		dev->hard_header_len += sizeof(struct ccci_header);
+#endif
+	} else {
+#ifdef ENABLE_WQ_GRO
+		dev->features |= NETIF_F_GRO;
+		dev->hw_features |= NETIF_F_GRO;
+#else
+		dev->hard_header_len += sizeof(struct ccci_header);
+#endif
+	}
+	dev->addr_len = ETH_ALEN;
+	dev->destructor = free_netdev;
+	dev->netdev_ops = &ccmni_netdev_ops;
+	random_ether_addr((u8 *) dev->dev_addr);
+}
+
+static int ccmni_init(int md_id, struct ccmni_ccci_ops *ccci_info)
 {
 	int i = 0, j = 0, ret = 0;
-	ccmni_ctl_block_t *ctlb = NULL;
-	ccmni_ctl_block_t *ctlb_irat_src = NULL;
-	ccmni_instance_t  *ccmni = NULL;
-	ccmni_instance_t  *ccmni_irat_src = NULL;
+	struct ccmni_ctl_block *ctlb = NULL;
+	struct ccmni_ctl_block *ctlb_irat_src = NULL;
+	struct ccmni_instance *ccmni = NULL;
+	struct ccmni_instance *ccmni_irat_src = NULL;
 	struct net_device *dev = NULL;
 
 	if (unlikely(ccci_info->md_ability & MODEM_CAP_CCMNI_DISABLE)) {
-		CCMNI_ERR_MSG(md_id, "no need init ccmni: md_ability=0x%08X\n", ccci_info->md_ability);
+		CCMNI_PR_ERR(md_id, "no need init ccmni: md_ability=0x%08X\n", ccci_info->md_ability);
 		return 0;
 	}
 
-	ctlb = kzalloc(sizeof(ccmni_ctl_block_t), GFP_KERNEL);
+	ctlb = kzalloc(sizeof(struct ccmni_ctl_block), GFP_KERNEL);
 	if (unlikely(ctlb == NULL)) {
-		CCMNI_ERR_MSG(md_id, "alloc ccmni ctl struct fail\n");
+		CCMNI_PR_ERR(md_id, "alloc ccmni ctl struct fail\n");
 		return -ENOMEM;
 	}
 
-	ctlb->ccci_ops = kzalloc(sizeof(ccmni_ccci_ops_t), GFP_KERNEL);
+	ctlb->ccci_ops = kzalloc(sizeof(struct ccmni_ccci_ops), GFP_KERNEL);
 	if (unlikely(ctlb->ccci_ops == NULL)) {
-		CCMNI_ERR_MSG(md_id, "alloc ccmni_ccci_ops struct fail\n");
+		CCMNI_PR_ERR(md_id, "alloc ccmni_ccci_ops struct fail\n");
 		ret = -ENOMEM;
 		goto alloc_mem_fail;
 	}
 
-	ccmni_ctl_blk[md_id] = ctlb;
-
-	memcpy(ctlb->ccci_ops, ccci_info, sizeof(ccmni_ccci_ops_t));
-
-	CCMNI_INF_MSG(md_id,
-		"ccmni_init: ccmni_num=%d, md_ability=0x%08x, irat_en=%08x, irat_md_id=%d, send_pkt=%p, get_ccmni_ch=%p, name=%s\n",
-		ctlb->ccci_ops->ccmni_num, ctlb->ccci_ops->md_ability,
-		(ctlb->ccci_ops->md_ability & MODEM_CAP_CCMNI_IRAT),
-		ctlb->ccci_ops->irat_md_id, ctlb->ccci_ops->send_pkt,
-		ctlb->ccci_ops->get_ccmni_ch, ctlb->ccci_ops->name);
+#if defined(CCMNI_MET_DEBUG)
+	if (met_tag_init() != 0)
+		CCMNI_INF_MSG(md_id, "ccmni_init:met tag init fail\n");
+#endif
 
-	ccmni_debug_file_init(md_id);
+	ccmni_ctl_blk[md_id] = ctlb;
+	memcpy(ctlb->ccci_ops, ccci_info, sizeof(struct ccmni_ccci_ops));
 
-		for (i = 0; i < ctlb->ccci_ops->ccmni_num; i++) {
-			/* allocate netdev */
-			if (ctlb->ccci_ops->md_ability & MODEM_CAP_CCMNI_MQ)
-				/* alloc multiple tx queue, 2 txq and 1 rxq */
-				dev = alloc_etherdev_mqs(sizeof(ccmni_instance_t), 2, 1);
-			else
-				dev = alloc_etherdev(sizeof(ccmni_instance_t));
-			if (unlikely(dev == NULL)) {
-				CCMNI_ERR_MSG(md_id, "alloc netdev fail\n");
-				ret = -ENOMEM;
-				goto alloc_netdev_fail;
-			}
+	for (i = 0; i < ctlb->ccci_ops->ccmni_num; i++) {
+		/* allocate netdev */
+		if (ctlb->ccci_ops->md_ability & MODEM_CAP_CCMNI_MQ) {
+			/* alloc multiple tx queue, 2 txq and 1 rxq */
+			dev = alloc_etherdev_mqs(sizeof(struct ccmni_instance), 2, 1);
+		} else {
+			dev = alloc_etherdev(sizeof(struct ccmni_instance));
+		}
+		if (unlikely(dev == NULL)) {
+			CCMNI_PR_ERR(md_id, "alloc netdev fail\n");
+			ret = -ENOMEM;
+			goto alloc_netdev_fail;
+		}
 
-			/* init net device */
-			dev->header_ops = NULL;
-			dev->mtu = CCMNI_MTU;
-			dev->tx_queue_len = CCMNI_TX_QUEUE;
-			dev->watchdog_timeo = CCMNI_NETDEV_WDT_TO;
-			dev->flags = (IFF_NOARP | IFF_BROADCAST) & /* ccmni is a pure IP device */
-					(~IFF_MULTICAST);	/* ccmni is P2P */
-			dev->features = NETIF_F_VLAN_CHALLENGED; /* not support VLAN */
-			if (ctlb->ccci_ops->md_ability & MODEM_CAP_SGIO) {
-				dev->features |= NETIF_F_SG;
-				dev->hw_features |= NETIF_F_SG;
-			}
-			if (ctlb->ccci_ops->md_ability & MODEM_CAP_NAPI) {
-#ifdef ENABLE_NAPI_GRO
-				dev->features |= NETIF_F_GRO;
-				dev->hw_features |= NETIF_F_GRO;
-#else
-				/*
-				 * check gro_list_prepare, GRO needs hard_header_len == ETH_HLEN.
-				 * CCCI header can use ethernet header and padding bytes' region.
-				 */
-				dev->hard_header_len += sizeof(struct ccci_header);
-#endif
-			} else {
-#ifdef ENABLE_WQ_GRO
-				dev->features |= NETIF_F_GRO;
-				dev->hw_features |= NETIF_F_GRO;
-#else
-				dev->hard_header_len += sizeof(struct ccci_header);
-#endif
-			}
-			dev->addr_len = ETH_ALEN; /* ethernet header size */
-			dev->destructor = free_netdev;
-			dev->netdev_ops = &ccmni_netdev_ops;
-			random_ether_addr((u8 *) dev->dev_addr);
-
-			sprintf(dev->name, "%s%d", ctlb->ccci_ops->name, i);
-			CCMNI_INF_MSG(md_id, "register netdev name: %s\n", dev->name);
-
-			/* init private structure of netdev */
-			ccmni = netdev_priv(dev);
-			ccmni->index = i;
-			ret = ccmni_inst_init(md_id, ccmni, dev);
-			if (ret) {
-				CCMNI_ERR_MSG(md_id, "initial ccmni instance fail\n");
-				goto alloc_netdev_fail;
-			}
-			ctlb->ccmni_inst[i] = ccmni;
+		/* init net device */
+		ccmni_dev_init(md_id, dev);
+		dev->type = ARPHRD_PPP;
 
-			/* register net device */
-			ret = register_netdev(dev);
-			if (ret) {
-				CCMNI_ERR_MSG(md_id, "CCMNI%d register netdev fail: %d\n", i, ret);
-				goto alloc_netdev_fail;
-			}
+		sprintf(dev->name, "%s%d", ctlb->ccci_ops->name, i);
 
-			CCMNI_DBG_MSG(ccmni->md_id, "CCMNI%d=%p, ctlb=%p, ctlb_ops=%p, dev=%p\n",
-				i, ccmni, ccmni->ctlb, ccmni->ctlb->ccci_ops, ccmni->dev);
+		/* init private structure of netdev */
+		ccmni = netdev_priv(dev);
+		ccmni->index = i;
+		ret = ccmni_inst_init(md_id, ccmni, dev);
+		if (ret) {
+			CCMNI_PR_ERR(md_id, "initial ccmni instance fail\n");
+			goto alloc_netdev_fail;
 		}
+		ctlb->ccmni_inst[i] = ccmni;
+
+		/* register net device */
+		ret = register_netdev(dev);
+		if (ret)
+			goto alloc_netdev_fail;
+	}
 
 	if ((ctlb->ccci_ops->md_ability & MODEM_CAP_CCMNI_IRAT) != 0) {
 		if (ctlb->ccci_ops->irat_md_id < 0 || ctlb->ccci_ops->irat_md_id >= MAX_MD_NUM) {
-			CCMNI_ERR_MSG(md_id, "md%d IRAT fail because invalid irat md(%d)\n",
-				md_id, ctlb->ccci_ops->irat_md_id);
+			CCMNI_PR_ERR(md_id, "md%d IRAT fail: invalid irat md(%d)\n",
+				      md_id, ctlb->ccci_ops->irat_md_id);
 			ret = -EINVAL;
 			goto alloc_mem_fail;
 		}
 
 		ctlb_irat_src = ccmni_ctl_blk[ctlb->ccci_ops->irat_md_id];
 		if (!ctlb_irat_src) {
-			CCMNI_ERR_MSG(md_id, "md%d IRAT fail because irat md%d ctlb is NULL\n",
-				md_id, ctlb->ccci_ops->irat_md_id);
+			CCMNI_PR_ERR(md_id, "md%d IRAT fail: irat md%d ctlb is NULL\n",
+				      md_id, ctlb->ccci_ops->irat_md_id);
 			ret = -EINVAL;
 			goto alloc_mem_fail;
 		}
 
 		if (unlikely(ctlb->ccci_ops->ccmni_num > ctlb_irat_src->ccci_ops->ccmni_num)) {
-			CCMNI_ERR_MSG(md_id, "IRAT fail because number of src(%d) and dest(%d) ccmni isn't equal\n",
-				ctlb_irat_src->ccci_ops->ccmni_num, ctlb->ccci_ops->ccmni_num);
+			CCMNI_PR_ERR(md_id, "IRAT fail: ccmni number not match(%d, %d)\n",
+				      ctlb_irat_src->ccci_ops->ccmni_num, ctlb->ccci_ops->ccmni_num);
 			ret = -EINVAL;
 			goto alloc_mem_fail;
 		}
 
 		for (i = 0; i < ctlb->ccci_ops->ccmni_num; i++) {
 			ccmni = ctlb->ccmni_inst[i];
-			ccmni_irat_src = kzalloc(sizeof(ccmni_instance_t), GFP_KERNEL);
+			ccmni_irat_src = kzalloc(sizeof(struct ccmni_instance), GFP_KERNEL);
 			if (unlikely(ccmni_irat_src == NULL)) {
-				CCMNI_ERR_MSG(md_id, "alloc ccmni_irat instance fail\n");
+				CCMNI_PR_ERR(md_id, "alloc ccmni_irat instance fail\n");
 				kfree(ccmni);
 				ret = -ENOMEM;
 				goto alloc_mem_fail;
@@ -884,12 +969,16 @@ static int ccmni_rx_callback(int md_id, int ccmni_idx, struct sk_buff *skb, void
 #endif
 	} else {
 #ifdef ENABLE_WQ_GRO
-		/* gro api should be called under preempt disable */
-		preempt_disable();
-		spin_lock_bh(ccmni->spinlock);
-		napi_gro_receive(ccmni->napi, skb);
-		spin_unlock_bh(ccmni->spinlock);
-		preempt_enable();
+		if (is_skb_gro(skb)) {
+			preempt_disable();
+			spin_lock_bh(ccmni->spinlock);
+			napi_gro_receive(ccmni->napi, skb);
+			ccmni_gro_flush(ccmni);
+			spin_unlock_bh(ccmni->spinlock);
+			preempt_enable();
+		} else {
+			netif_rx_ni(skb);
+		}
 #else
 		if (!in_interrupt())
 			netif_rx_ni(skb);
@@ -920,46 +1022,42 @@ static int ccmni_rx_callback(int md_id, int ccmni_idx, struct sk_buff *skb, void
 
 static void ccmni_md_state_callback(int md_id, int ccmni_idx, MD_STATE state, int is_ack)
 {
-	ccmni_ctl_block_t *ctlb = ccmni_ctl_blk[md_id];
-	ccmni_instance_t *ccmni = NULL;
-	ccmni_instance_t *ccmni_tmp = NULL;
+	struct ccmni_ctl_block *ctlb = ccmni_ctl_blk[md_id];
+	struct ccmni_instance *ccmni = NULL;
+	struct ccmni_instance *ccmni_tmp = NULL;
 	struct net_device *dev = NULL;
 	struct netdev_queue *net_queue = NULL;
 	int i = 0;
 
 	if (unlikely(ctlb == NULL)) {
-		CCMNI_ERR_MSG(md_id, "invalid ccmni ctrl struct when ccmni_idx=%d md_sta=%d\n", ccmni_idx, state);
+		CCMNI_DBG_MSG(md_id, "invalid ccmni ctrl struct when ccmni_idx=%d md_sta=%d\n", ccmni_idx, state);
 		return;
 	}
 
 	ccmni_tmp = ctlb->ccmni_inst[ccmni_idx];
 	dev = ccmni_tmp->dev;
-	ccmni = (ccmni_instance_t *)netdev_priv(dev);
+	ccmni = (struct ccmni_instance *)netdev_priv(dev);
 
 	if ((state != RX_IRQ) && (state != RX_FLUSH) &&
 		(state != TX_IRQ) && (state != TX_FULL) &&
 		(atomic_read(&ccmni->usage) > 0)) {
-		CCMNI_INF_MSG(md_id, "md_state_cb: CCMNI%d, md_sta=%d, usage=%d\n",
+		CCMNI_DBG_MSG(md_id, "md_state_cb: CCMNI%d, md_sta=%d, usage=%d\n",
 			ccmni_idx, state, atomic_read(&ccmni->usage));
 	}
 
 	switch (state) {
 	case READY:
-		/* Only do carrier on for ccmni-lan.
-		 * don't carrire on other interface, MD data link may be not ready. carrirer on it in ccmni_open
-		 */
-		if (IS_CCMNI_LAN(ccmni->dev))
-			netif_carrier_on(ccmni->dev);
-
+		/*don't carrire on here, MD data link may be not ready. carrirer on it in ccmni_open*/
+		/*netif_carrier_on(ccmni->dev);*/
 		for (i = 0; i < 2; i++) {
 			ccmni->tx_seq_num[i] = 0;
 			ccmni->tx_full_cnt[i] = 0;
 			ccmni->tx_irq_cnt[i] = 0;
 			ccmni->tx_full_tick[i] = 0;
-			ccmni->tx_irq_tick[i] = 0;
 			ccmni->flags[i] &= ~CCMNI_TX_PRINT_F;
 		}
 		ccmni->rx_seq_num = 0;
+		ccmni->rx_gro_cnt = 0;
 		break;
 
 	case EXCEPTION:
@@ -967,12 +1065,11 @@ static void ccmni_md_state_callback(int md_id, int ccmni_idx, MD_STATE state, in
 	case WAITING_TO_STOP:
 		netif_carrier_off(ccmni->dev);
 		break;
-
 #ifdef ENABLE_WQ_GRO
 	case RX_FLUSH:
-		/* gro api should be called under preempt disable */
 		preempt_disable();
 		spin_lock_bh(ccmni->spinlock);
+		ccmni->rx_gro_cnt++;
 		napi_gro_flush(ccmni->napi, false);
 		spin_unlock_bh(ccmni->spinlock);
 		preempt_enable();
@@ -1028,7 +1125,7 @@ static void ccmni_md_state_callback(int md_id, int ccmni_idx, MD_STATE state, in
 			if (time_after(jiffies, ccmni->tx_full_tick[is_ack] + 4)) {
 				ccmni->tx_full_tick[is_ack] = jiffies;
 				ccmni->flags[is_ack] |= CCMNI_TX_PRINT_F;
-				CCMNI_INF_MSG(md_id, "%s(%d), idx=%d, md_sta=TX_FULL, ack=%d, cnt(%u, %u)\n",
+				CCMNI_DBG_MSG(md_id, "%s(%d), idx=%d, hif_sta=TX_FULL, ack=%d, cnt(%u, %u)\n",
 					ccmni->dev->name, atomic_read(&ccmni->usage), ccmni->index,
 					is_ack, ccmni->tx_full_cnt[is_ack], ccmni->tx_irq_cnt[is_ack]);
 			}
@@ -1062,7 +1159,7 @@ static void ccmni_dump(int md_id, int ccmni_idx, unsigned int flag)
 
 	dev = ccmni_tmp->dev;
 	/* ccmni diff from ccmni_tmp for MD IRAT */
-	ccmni = (ccmni_instance_t *)netdev_priv(dev);
+	ccmni = (struct ccmni_instance *)netdev_priv(dev);
 	dev_queue = netdev_get_tx_queue(dev, 0);
 	if (ctlb->ccci_ops->md_ability & MODEM_CAP_CCMNI_MQ) {
 		ack_queue = netdev_get_tx_queue(dev, CCMNI_TXQ_FAST);
@@ -1071,28 +1168,29 @@ static void ccmni_dump(int md_id, int ccmni_idx, unsigned int flag)
 		/* stats.rx_dropped is dropped in ccmni, dev->rx_dropped is dropped in net device layer */
 		/* stats.tx_packets is count by ccmni, bstats.packets is count by qdisc in net device layer */
 		CCMNI_INF_MSG(md_id,
-			"%s(%d,%d), irat_MD%d, rx=(%ld,%ld), tx=(%ld,%d,%d), txq_len=(%d,%d), tx_drop=(%ld,%d,%d), rx_drop=(%ld,%ld), tx_busy=(%ld,%ld), sta=(0x%lx,0x%x,0x%lx,0x%lx)\n",
-			dev->name, atomic_read(&ccmni->usage), atomic_read(&ccmni_tmp->usage), (ccmni->md_id+1),
-			dev->stats.rx_packets, dev->stats.rx_bytes,
-			dev->stats.tx_packets, qdisc->bstats.packets, ack_qdisc->bstats.packets,
-			qdisc->q.qlen, ack_qdisc->q.qlen,
-			dev->stats.tx_dropped, qdisc->qstats.drops, ack_qdisc->qstats.drops,
-			dev->stats.rx_dropped, atomic_long_read(&dev->rx_dropped),
-			ccmni->tx_busy_cnt[0], ccmni->tx_busy_cnt[1],
-			dev->state, dev->flags, dev_queue->state, ack_queue->state);
+			      "%s(%d,%d), irat_MD%d, rx=(%ld,%ld,%d), tx=(%ld,%d,%d), txq_len=(%d,%d), tx_drop=(%ld,%d,%d), rx_drop=(%ld,%ld), tx_busy=(%ld,%ld), sta=(0x%lx,0x%x,0x%lx,0x%lx)\n",
+			      dev->name, atomic_read(&ccmni->usage), atomic_read(&ccmni_tmp->usage), (ccmni->md_id + 1),
+			      dev->stats.rx_packets, dev->stats.rx_bytes, ccmni->rx_gro_cnt,
+			      dev->stats.tx_packets, qdisc->bstats.packets, ack_qdisc->bstats.packets,
+			      qdisc->q.qlen, ack_qdisc->q.qlen,
+			      dev->stats.tx_dropped, qdisc->qstats.drops, ack_qdisc->qstats.drops,
+			      dev->stats.rx_dropped, atomic_long_read(&dev->rx_dropped),
+			      ccmni->tx_busy_cnt[0], ccmni->tx_busy_cnt[1],
+			      dev->state, dev->flags, dev_queue->state, ack_queue->state);
 	} else
 		CCMNI_INF_MSG(md_id,
-			"%s(%d,%d), irat_MD%d, rx=(%ld,%ld), tx=(%ld,%ld), txq_len=%d, tx_drop=(%ld,%d), rx_drop=(%ld,%ld), tx_busy=(%ld,%ld), sta=(0x%lx,0x%x,0x%lx)\n",
-			dev->name, atomic_read(&ccmni->usage), atomic_read(&ccmni_tmp->usage), (ccmni->md_id+1),
-			dev->stats.rx_packets, dev->stats.rx_bytes, dev->stats.tx_packets, dev->stats.tx_bytes,
-			dev->qdisc->q.qlen, dev->stats.tx_dropped, dev->qdisc->qstats.drops, dev->stats.rx_dropped,
-			atomic_long_read(&dev->rx_dropped), ccmni->tx_busy_cnt[0], ccmni->tx_busy_cnt[1],
-			dev->state, dev->flags, dev_queue->state);
+			      "%s(%d,%d), irat_MD%d, rx=(%ld,%ld,%d), tx=(%ld,%ld), txq_len=%d, tx_drop=(%ld,%d), rx_drop=(%ld,%ld), tx_busy=(%ld,%ld), sta=(0x%lx,0x%x,0x%lx)\n",
+			      dev->name, atomic_read(&ccmni->usage), atomic_read(&ccmni_tmp->usage), (ccmni->md_id + 1),
+			      dev->stats.rx_packets, dev->stats.rx_bytes, ccmni->rx_gro_cnt,
+			      dev->stats.tx_packets, dev->stats.tx_bytes,
+			      dev->qdisc->q.qlen, dev->stats.tx_dropped, dev->qdisc->qstats.drops,
+			      dev->stats.rx_dropped, atomic_long_read(&dev->rx_dropped), ccmni->tx_busy_cnt[0],
+			      ccmni->tx_busy_cnt[1], dev->state, dev->flags, dev_queue->state);
 }
 
 static void ccmni_dump_rx_status(int md_id, unsigned long long *status)
 {
-	ccmni_ctl_block_t *ctlb = ccmni_ctl_blk[md_id];
+	struct ccmni_ctl_block *ctlb = ccmni_ctl_blk[md_id];
 
 	status[0] = ctlb->net_rx_delay[0];
 	status[1] = ctlb->net_rx_delay[1];
